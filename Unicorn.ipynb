{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Files\n",
    "Uni_train = pd.read_csv(\"C:/Users/admn/Downloads/Unicorn/Train.csv\")\n",
    "Uni_test = pd.read_csv(\"C:/Users/admn/Downloads/Unicorn/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Copy\n",
    "Uni_train_copy = Uni_train.copy()\n",
    "Uni_test_copy = Uni_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ID', u'Datetime', u'Count'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uni_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ID', u'Datetime'], dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uni_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           int64\n",
       "Datetime    object\n",
       "Count        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Datatypes\n",
    "Uni_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           int64\n",
       "Datetime    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uni_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18288, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uni_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5112, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uni_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-def6ebcadb32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#To Count No of Passengers on year,month,day and hour basis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUni_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUni_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUni_train_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUni_test_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\admn\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5061\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[0;32m   5062\u001b[0m                 name in self._accessors):\n\u001b[1;32m-> 5063\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5064\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\admn\\Anaconda2\\lib\\site-packages\\pandas\\core\\accessor.pyc\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\admn\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexes\\accessors.pyc\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mpass\u001b[0m  \u001b[1;31m# we raise an attribute error anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         raise AttributeError(\"Can only use .dt accessor with datetimelike \"\n\u001b[0m\u001b[0;32m    325\u001b[0m                              \"values\")\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "#Convert Datatime into datatime format\n",
    "Uni_train['Datetime'] = pd.to_datetime(Uni_train.Datetime,format='%d-%m-%Y %H:%M')\n",
    "Uni_test['Datetime'] = pd.to_datetime(Uni_test.Datetime,format='%d-%m-%Y %H:%M')\n",
    "#Uni_test_copy['Datetime'] = pd.to_datetime(Uni_test_copy.Datetime,format='%d-%m-%Y %H:%M')\n",
    "\n",
    "\n",
    "#To Count No of Passengers on year,month,day and hour basis\n",
    "for i in(Uni_train, Uni_test, Uni_train_copy, Uni_test_copy):\n",
    "    i['year']=i.Datetime.dt.year\n",
    "    i['month']=i.Datetime.dt.month\n",
    "    i['day']=i.Datetime.dt.day\n",
    "    i['hour']=i.Datetime.dt.hour\n",
    "    \n",
    "#To calculate weekdays also\n",
    "Uni_train['day of week']=Uni_train['Datetime'].dt.dayofweek \n",
    "temp = Uni_train['Datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assin 1 if weekend and O if not\n",
    "def applyer(row):\n",
    "    if row.dayofweek == 5 or row.dayofweek == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "temp2 = Uni_train['Datetime'].apply(applyer) \n",
    "Uni_train['weekend']=temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "Uni_train.index = Uni_train['Datetime']\n",
    "df = Uni_train.drop(['ID'], axis =1)\n",
    "ts = df['Count']\n",
    "plt.figure(figsize=(16,8)) \n",
    "plt.plot(ts, label='Passenger Count') \n",
    "plt.title('Time Series') \n",
    "plt.xlabel(\"Time(year-month)\") \n",
    "plt.ylabel(\"Passenger count\") \n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking hupothesis\n",
    "Uni_train.groupby('year')['Count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_train.groupby('month')['Count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=Uni_train.groupby(['year', 'month'])['Count'].mean() \n",
    "temp.plot(figsize=(15,5), title= 'Passenger Count(Monthwise)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_train.groupby('day')['Count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_train.groupby('hour')['Count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_train.groupby('weekend')['Count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_train.groupby('day of week')['Count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop ID variable\n",
    "Uni_train = Uni_train.drop(['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating hour with day, week and month to reduce noise\n",
    "Uni_train.Timestamp = pd.to_datetime(Uni_train.Datetime,format='%d-%m-%Y %H:%M') \n",
    "Uni_train.index = Uni_train.Timestamp\n",
    "\n",
    "# Hourly time series \n",
    "hourly = Uni_train.resample('H').mean()\n",
    "\n",
    "# Converting to daily mean \n",
    "daily = Uni_train.resample('D').mean() \n",
    "# Converting to weekly mean \n",
    "weekly = Uni_train.resample('W').mean() \n",
    "# Converting to monthly mean \n",
    "monthly = Uni_train.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,1) \n",
    "hourly.Count.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0])\n",
    "daily.Count.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1]) \n",
    "weekly.Count.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2])\n",
    "monthly.Count.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3]) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test File\n",
    "Uni_test.Timestamp = pd.to_datetime(Uni_test.Datetime,format='%d-%m-%Y %H:%M') \n",
    "Uni_test.index = Uni_test.Timestamp  \n",
    "\n",
    "# Converting to daily mean \n",
    "Uni_test = Uni_test.resample('D').mean() \n",
    "\n",
    "#Train File\n",
    "Uni_train.Timestamp = pd.to_datetime(Uni_train.Datetime,format='%d-%m-%Y %H:%M') \n",
    "Uni_train.index = Uni_train.Timestamp\n",
    "\n",
    "# Converting to daily mean \n",
    "Uni_train = Uni_train.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the Train File\n",
    "Uni_Train=Uni_train.ix['2012-08-25':'2014-06-24'] \n",
    "Uni_valid=Uni_train.ix['2014-06-25':'2014-09-25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_Train.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='train')\n",
    "Uni_valid.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='valid') \n",
    "plt.xlabel(\"Datetime\") \n",
    "plt.ylabel(\"Passenger count\") \n",
    "plt.legend(loc='best') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forecasting Using Naive Bayes\n",
    "#Let’s make predictions using naive approach for the validation set.\n",
    "\n",
    "dd= np.asarray(Uni_Train.Count) \n",
    "y_hat = Uni_valid.copy() \n",
    "y_hat['naive'] = dd[len(dd)-1] \n",
    "plt.figure(figsize=(12,8)) \n",
    "plt.plot(Uni_Train.index, Uni_Train['Count'], label='Train') \n",
    "plt.plot(Uni_valid.index, Uni_valid['Count'], label='Valid') \n",
    "plt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast') \n",
    "plt.legend(loc='best') \n",
    "plt.title(\"Naive Forecast\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now calculate RMSE to check the accuracy of our model on validation data set.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt \n",
    "rms = sqrt(mean_squared_error(Uni_valid.Count, y_hat.naive)) \n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving Average\n",
    "#Here the predictions are made on the basis of the average of last few points instead of taking all the previously known values.\n",
    "\n",
    "#ets try the rolling mean for last 10, 20, 50 days and visualize the results.\n",
    "\n",
    "y_hat_avg = Uni_valid.copy()\n",
    "\n",
    "#Calculating Average of last 10 Transactions\n",
    "y_hat_avg['moving_avg_forecast'] = Uni_Train['Count'].rolling(10).mean().iloc[-1]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(Uni_Train['Count'], label = 'Train')\n",
    "plt.plot(Uni_valid['Count'],label = 'Valid')\n",
    "plt.plot(y_hat_avg['moving_avg_forecast'], label = 'Moving Average Forecast using 10 observations')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Calculating With last 20 transactions\n",
    "y_hat_avg = Uni_valid.copy()\n",
    "y_hat_avg['moving_avg_forecast'] = Uni_Train['Count'].rolling(20).mean().iloc[-1]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(Uni_Train['Count'], label = 'Train')\n",
    "plt.plot(Uni_valid['Count'],label = 'Valid')\n",
    "plt.plot(y_hat_avg['moving_avg_forecast'], label = 'Moving Average Forecast using 20 observations')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Calculating with last 50 transactions\n",
    "y_hat_avg = Uni_valid.copy()\n",
    "y_hat_avg['moving_avg_forecast'] = Uni_Train['Count'].rolling(50).mean().iloc[-1]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(Uni_Train['Count'], label = 'Train')\n",
    "plt.plot(Uni_valid['Count'],label = 'Valid')\n",
    "plt.plot(y_hat_avg['moving_avg_forecast'], label = 'Moving Average Forecast using 50 observations')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(Uni_valid.Count, y_hat_avg.moving_avg_forecast)) \n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Exponential Smoothing\n",
    "#Here the predictions are made by assigning larger weight to the recent values and lesser weight to the old values.\n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \n",
    "y_hat_avg = Uni_valid.copy() \n",
    "fit2 = SimpleExpSmoothing(np.asarray(Uni_Train['Count'])).fit(smoothing_level=0.6,optimized=False)\n",
    "y_hat_avg['SES'] = fit2.forecast(len(Uni_valid)) \n",
    "plt.figure(figsize=(16,8)) \n",
    "plt.plot(Uni_Train['Count'], label='Train') \n",
    "plt.plot(Uni_valid['Count'], label='Valid') \n",
    "plt.plot(y_hat_avg['SES'], label='SES') \n",
    "plt.legend(loc='best') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(Uni_valid.Count, y_hat_avg.SES)) \n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holt’s Linear Trend Model\n",
    "import statsmodels.api as sm \n",
    "sm.tsa.seasonal_decompose(Uni_Train.Count).plot() \n",
    "result = sm.tsa.stattools.adfuller(Uni_train.Count) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An increasing trend can be seen in the dataset, so now we will make a model based on the trend.\n",
    "\n",
    "y_hat_avg = Uni_valid.copy() \n",
    "fit1 = Holt(np.asarray(Uni_Train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1) \n",
    "y_hat_avg['Holt_linear'] = fit1.forecast(len(Uni_valid)) \n",
    "plt.figure(figsize=(16,8)) \n",
    "plt.plot(Uni_Train['Count'], label='Train') \n",
    "plt.plot(Uni_valid['Count'], label='Valid') \n",
    "plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear') \n",
    "plt.legend(loc='best') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(Uni_valid.Count, y_hat_avg.Holt_linear)) \n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Holt’s Linear Trend Model on daily time series\n",
    "submission=pd.read_csv(\"C:/Users/admn/Downloads/Unicorn/Submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=fit1.forecast(len(Uni_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_test['prediction']=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the hourly ratio of count \n",
    "Uni_train_copy['ratio']=Uni_train_copy['Count']/Uni_train_copy['Count'].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the hourly ratio \n",
    "temp=Uni_train_copy.groupby(['hour'])['ratio'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby to csv format \n",
    "pd.DataFrame(temp, columns=['hour','ratio']).to_csv('GROUPby.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2=pd.read_csv(\"GROUPby.csv\") \n",
    "temp2=temp2.drop('hour.1',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Test and test_original on day, month and year \n",
    "merge=pd.merge(Uni_test, Uni_test_copy, on=('day','month', 'year'), how='left') \n",
    "merge['hour'] = merge['hour_y'] \n",
    "merge=merge.drop(['year', 'month', 'Datetime','hour_x','hour_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting by merging merge and temp2 \n",
    "prediction=pd.merge(merge, temp2, on='hour', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the ratio to the original scale \n",
    "prediction['Count']=prediction['prediction']*prediction['ratio']*24 \n",
    "prediction['ID']=prediction['ID_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s drop all other features from the submission file and keep ID and Count only.\n",
    "\n",
    "submission=prediction.drop(['ID_x', 'day', 'ID_y','prediction','hour', 'ratio'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the final submission to csv format \n",
    "pd.DataFrame(submission, columns=['ID','Count']).to_csv('Holt linear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################Holt winter’s model on daily time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_avg = Uni_valid.copy() \n",
    "fit1 = ExponentialSmoothing(np.asarray(Uni_Train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit() \n",
    "y_hat_avg['Holt_Winter'] = fit1.forecast(len(Uni_valid)) \n",
    "plt.figure(figsize=(16,8)) \n",
    "plt.plot(Uni_Train['Count'], label='Train') \n",
    "plt.plot(Uni_valid['Count'], label='Valid') \n",
    "plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter') \n",
    "plt.legend(loc='best') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(Uni_valid.Count, y_hat_avg.Holt_Winter))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=fit1.forecast(len(Uni_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will convert these daily passenger count into hourly passenger count using the same approach which we followed above.\n",
    "\n",
    "Uni_test['prediction']=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Test and test_original on day, month and year \n",
    "merge=pd.merge(Uni_test, Uni_test_copy, on=('Day','month', 'year'), how='left') \n",
    "merge['hour']=merge['hour_y'] \n",
    "merge=merge.drop(['year', 'month', 'Datetime','hour_x','hour_y'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting by merging merge and temp2 \n",
    "prediction=pd.merge(merge, temp2, on='hour', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the ratio to the original scale \n",
    "prediction['Count']=prediction['prediction']*prediction['ratio']*24\n",
    "#Let’s drop all features other than ID and Count\n",
    "\n",
    "prediction['ID']=prediction['ID_y'] \n",
    "submission=prediction.drop(['day','hour','ratio','prediction', 'ID_x', 'ID_y'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the final submission to csv format\n",
    "pd.DataFrame(submission, columns=['ID','Count']).to_csv('Holt winters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determining rolling statistics\n",
    "    rolmean = timeseries.rolling(window = 24).mean()\n",
    "    rolSTD = timeseries.rolling(window = 24).std()\n",
    "    \n",
    "    #Plot Rolling statistics\n",
    "    orig = plt.plot(timeseries, color = 'blue', label = 'Original')\n",
    "    mean = plt.plot(rolmean, color = 'Red', label = 'Mean')\n",
    "    std = plt.plot(rolSTD, color = 'Black', label = 'Std')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title(\"Rolling Mean and STD deviation\")\n",
    "    plt.show(block=False)   \n",
    "    \n",
    "    \n",
    "    #Dickey Fuller Test\n",
    "    print(\"Results of Dickey Fuller test\")\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfop = pd.Series(dftest[0:4], index=['Test-statistics', 'pvalue', '#Lags used', 'No of observations'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfop['critical value (%s)'%key] = value\n",
    "    print(dfop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams \n",
    "rcParams['figure.figsize'] = 20,10\n",
    "test_stationarity(Uni_train_copy['Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Trend\n",
    "Train_log = np.log(Uni_Train['Count']) \n",
    "valid_log = np.log(Uni_valid['Count'])\n",
    "moving_avg = Train_log.rolling(24).mean()\n",
    "plt.plot(Train_log) \n",
    "plt.plot(moving_avg, color = 'red') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing this Trend\n",
    "train_log_moving_avg_diff = Train_log - moving_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we took the average of 24 values, rolling mean is not defined for the first 23 values. So let’s drop those null values.\n",
    "\n",
    "train_log_moving_avg_diff.dropna(inplace = True) \n",
    "test_stationarity(train_log_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that the Test Statistic is very smaller as compared to the Critical Value. So, we can be confident that the trend is almost removed.\n",
    "\n",
    "#Let’s now stabilize the mean of the time series which is also a requirement for a stationary time series.\n",
    "\n",
    "#Differencing can help to make the series stable and eliminate the trend.\n",
    "train_log_diff = Train_log - Train_log.shift(1) \n",
    "test_stationarity(train_log_diff.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Seasonality\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "decomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values, freq = 24) \n",
    "\n",
    "trend = decomposition.trend \n",
    "seasonal = decomposition.seasonal \n",
    "residual = decomposition.resid \n",
    "\n",
    "plt.subplot(411) \n",
    "plt.plot(Train_log, label='Original') \n",
    "plt.legend(loc='best') \n",
    "plt.subplot(412) \n",
    "plt.plot(trend, label='Trend') \n",
    "plt.legend(loc='best') \n",
    "plt.subplot(413) \n",
    "plt.plot(seasonal,label='Seasonality') \n",
    "plt.legend(loc='best') \n",
    "plt.subplot(414) \n",
    "plt.plot(residual, label='Residuals') \n",
    "plt.legend(loc='best') \n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Stationarity in Residuals\n",
    "train_log_decompose = pd.DataFrame(residual) \n",
    "train_log_decompose['date'] = Train_log.index \n",
    "train_log_decompose.set_index('date', inplace = True) \n",
    "train_log_decompose.dropna(inplace=True) \n",
    "test_stationarity(train_log_decompose[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forecasting the time series using ARIMA\n",
    "\n",
    "#First of all we will fit the ARIMA model on our time series for that we have to find the optimized values for the p,d,q parameters.\n",
    "\n",
    "#To find the optimized values of these parameters, we will use ACF(Autocorrelation Function) and PACF(Partial Autocorrelation Function) graph.\n",
    "\n",
    "#PACF measures the correlation between the TimeSeries with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons.\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf \n",
    "lag_acf = acf(train_log_diff.dropna(), nlags=25) \n",
    "lag_pacf = pacf(train_log_diff.dropna(), nlags=25, method='ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_acf) \n",
    "plt.axhline(y=0,linestyle='--',color='gray') \n",
    "plt.axhline(y=-1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \n",
    "plt.axhline(y=1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \n",
    "plt.title('Autocorrelation Function') \n",
    "plt.show() \n",
    "plt.plot(lag_pacf) \n",
    "plt.axhline(y=0,linestyle='--',color='gray') \n",
    "plt.axhline(y=-1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \n",
    "plt.axhline(y=1.96/np.sqrt(len(train_log_diff.dropna())),linestyle='--',color='gray') \n",
    "plt.title('Partial Autocorrelation Function') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(Train_log, order=(2, 1, 0))  # here the q value is zero since it is just the AR model \n",
    "results_AR = model.fit(disp=-1)  \n",
    "plt.plot(train_log_diff.dropna(), label='original') \n",
    "plt.plot(results_AR.fittedvalues, color='red', label='predictions') \n",
    "plt.legend(loc='best') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets plot the validation curve for AR model.\n",
    "\n",
    "#We have to change the scale of the model to the original scale.\n",
    "\n",
    "#First step would be to store the predicted results as a separate series and observe it.\n",
    "\n",
    "AR_predict=results_AR.predict(start=\"2014-06-25\", end=\"2014-09-25\") \n",
    "AR_predict=AR_predict.cumsum().shift().fillna(0) \n",
    "AR_predict1=pd.Series(np.ones(Uni_valid.shape[0]) * np.log(Uni_valid['Count'])[0], index = Uni_valid.index) \n",
    "AR_predict1=AR_predict1.add(AR_predict,fill_value=0) \n",
    "AR_predict = np.exp(AR_predict1)\n",
    "plt.plot(Uni_valid['Count'], label = \"Valid\") \n",
    "plt.plot(AR_predict, color = 'red', label = \"Predict\") \n",
    "plt.legend(loc= 'best') \n",
    "plt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, Uni_valid['Count']))/Uni_valid.shape[0])) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ma Model\n",
    "#The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.\n",
    "\n",
    "model = ARIMA(Train_log, order=(0, 1, 2))  # here the p value is zero since it is just the MA model \n",
    "results_MA = model.fit(disp=-1)  \n",
    "plt.plot(train_log_diff.dropna(), label='original') \n",
    "plt.plot(results_MA.fittedvalues, color='red', label='prediction') \n",
    "plt.legend(loc='best') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_predict=results_MA.predict(start=\"2014-06-25\", end=\"2014-09-25\") \n",
    "MA_predict=MA_predict.cumsum().shift().fillna(0) \n",
    "MA_predict1=pd.Series(np.ones(Uni_valid.shape[0]) * np.log(Uni_valid['Count'])[0], index = Uni_valid.index) \n",
    "MA_predict1=MA_predict1.add(MA_predict,fill_value=0) \n",
    "MA_predict = np.exp(MA_predict1)\n",
    "plt.plot(Uni_valid['Count'], label = \"Valid\") \n",
    "plt.plot(MA_predict, color = 'red', label = \"Predict\") \n",
    "plt.legend(loc= 'best') \n",
    "plt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, Uni_valid['Count']))/Uni_valid.shape[0])) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining_Model\n",
    "model = ARIMA(Train_log, order=(2, 1, 2))  \n",
    "results_ARIMA = model.fit(disp=-1)  \n",
    "plt.plot(train_log_diff.dropna(),  label='original') \n",
    "plt.plot(results_ARIMA.fittedvalues, color='red', label='predicted') \n",
    "plt.legend(loc='best') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s define a function which can be used to change the scale of the model to the original scale.\n",
    "\n",
    "def check_prediction_diff(predict_diff, given_set):\n",
    "    predict_diff= predict_diff.cumsum().shift().fillna(0)\n",
    "    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Count'])[0], index = given_set.index)\n",
    "    predict_log = predict_base.add(predict_diff,fill_value=0)\n",
    "    predict = np.exp(predict_log)\n",
    "\n",
    "    plt.plot(given_set['Count'], label = \"Given set\")\n",
    "    plt.plot(predict, color = 'red', label = \"Predict\")\n",
    "    plt.legend(loc= 'best')\n",
    "    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))/given_set.shape[0]))\n",
    "    plt.show()\n",
    "\n",
    "def check_prediction_log(predict_log, given_set):\n",
    "    predict = np.exp(predict_log)\n",
    " \n",
    "    plt.plot(given_set['Count'], label = \"Given set\")\n",
    "    plt.plot(predict, color = 'red', label = \"Predict\")\n",
    "    plt.legend(loc= 'best')\n",
    "    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))/given_set.shape[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s predict the values for validation set.\n",
    "\n",
    "ARIMA_predict_diff=results_ARIMA.predict(start=\"2014-06-25\", end=\"2014-09-25\")\n",
    "check_prediction_diff(ARIMA_predict_diff, Uni_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
